{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d2f3e4a"
   },
   "source": [
    "# AutoPilotTuiML testing funcs (local version)\n",
    "The main goal of this notebook is only to debug and test fucntions from the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ec3b08d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "  \n",
    "    \n",
    "sys.path.insert(0, '/Users/Tabe/Desktop/Projects/GitHub/autopilot/tuiautopilotml')\n",
    "\n",
    "from wrappers import *\n",
    "from autopilot_mode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/Users/Tabe/Desktop/Projects/GitHub/autopilot/sample_data/travelers.csv'\n",
    "path2 = '/Users/Tabe/Desktop/Projects/GitHub/autopilot/sample_data/trips.csv'\n",
    "path3 = '/Users/Tabe/Desktop/Projects/GitHub/autopilot/sample_data/product_sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0f55f1b2"
   },
   "outputs": [],
   "source": [
    "travelers = pd.read_csv(path1, sep='\\t' , engine = 'python' )\n",
    "trips = pd.read_csv(path2, sep='\\t' , engine = 'python' )\n",
    "trips = trips.rename(columns={'traveler_id':'id'})\n",
    "products = pd.read_csv(path3, sep='\\t' , engine = 'python' )\n",
    "trips_travelers = pd.merge(trips , travelers , how = 'left' , on = 'id' )\n",
    "trips_travelers_p = pd.merge(trips_travelers ,products , how = 'left' , on = 'reservation_id' )\n",
    "trips_travelers_p['bought_product'] =  trips_travelers_p['bought_product'].apply(lambda x: 1 if x == 1.0 else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reservation_id</th>\n",
       "      <th>reservation_date</th>\n",
       "      <th>package_type</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>departure_code</th>\n",
       "      <th>arrival_code</th>\n",
       "      <th>arrival_country</th>\n",
       "      <th>return_date</th>\n",
       "      <th>carrier_id</th>\n",
       "      <th>no_of_adults</th>\n",
       "      <th>no_of_kids</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>gender_id</th>\n",
       "      <th>country_id</th>\n",
       "      <th>bought_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8066822</td>\n",
       "      <td>3dae89e2133e2e6734c0eca07846ff2e57079ba7780005...</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>STP</td>\n",
       "      <td>2017-09-14</td>\n",
       "      <td>OSL</td>\n",
       "      <td>SMI</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2017-09-21</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1963-10-04</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7612471</td>\n",
       "      <td>33099e82865f00c713987276da184bee2e742757b05f80...</td>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>STP</td>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>GOT</td>\n",
       "      <td>AHO</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1980-02-02</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6635207</td>\n",
       "      <td>19edda1fedb1a1e6777a055238febb7ecc64e87b60583e...</td>\n",
       "      <td>2018-08-16</td>\n",
       "      <td>STP</td>\n",
       "      <td>2018-09-09</td>\n",
       "      <td>ARN</td>\n",
       "      <td>HER</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1949-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8668860</td>\n",
       "      <td>27798ab2976d9db73fefb47140a441af478c1fff10d26e...</td>\n",
       "      <td>2018-07-13</td>\n",
       "      <td>STP</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>GOT</td>\n",
       "      <td>AYT</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>2018-08-09</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-03-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6921503</td>\n",
       "      <td>d217195fd6a986fd7ead59ca9081d46f3de8e9a52779b5...</td>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>STP</td>\n",
       "      <td>2017-10-17</td>\n",
       "      <td>ARN</td>\n",
       "      <td>CHQ</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1965-05-22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                     reservation_id  \\\n",
       "0  8066822  3dae89e2133e2e6734c0eca07846ff2e57079ba7780005...   \n",
       "1  7612471  33099e82865f00c713987276da184bee2e742757b05f80...   \n",
       "2  6635207  19edda1fedb1a1e6777a055238febb7ecc64e87b60583e...   \n",
       "3  8668860  27798ab2976d9db73fefb47140a441af478c1fff10d26e...   \n",
       "4  6921503  d217195fd6a986fd7ead59ca9081d46f3de8e9a52779b5...   \n",
       "\n",
       "  reservation_date package_type departure_date departure_code arrival_code  \\\n",
       "0       2017-09-04          STP     2017-09-14            OSL          SMI   \n",
       "1       2017-01-25          STP     2017-06-16            GOT          AHO   \n",
       "2       2018-08-16          STP     2018-09-09            ARN          HER   \n",
       "3       2018-07-13          STP     2018-07-26            GOT          AYT   \n",
       "4       2017-09-29          STP     2017-10-17            ARN          CHQ   \n",
       "\n",
       "  arrival_country return_date  carrier_id  no_of_adults  no_of_kids  \\\n",
       "0          Greece  2017-09-21         246             2           0   \n",
       "1           Italy  2017-06-23         246             2           2   \n",
       "2          Greece  2018-09-16         246             2           1   \n",
       "3          Turkey  2018-08-09         246             2           1   \n",
       "4          Greece  2017-10-24         246             2           1   \n",
       "\n",
       "    birthdate  gender_id  country_id  bought_product  \n",
       "0  1963-10-04          3           2               0  \n",
       "1  1980-02-02          3           1               1  \n",
       "2  1949-08-29          1           1               0  \n",
       "3  1960-03-10          1           1               0  \n",
       "4  1965-05-22          1           1               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_travelers_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "reservation_id       0\n",
       "reservation_date     0\n",
       "package_type         0\n",
       "departure_date       0\n",
       "departure_code       0\n",
       "arrival_code         0\n",
       "arrival_country      0\n",
       "return_date          0\n",
       "carrier_id           0\n",
       "no_of_adults         0\n",
       "no_of_kids           0\n",
       "birthdate           80\n",
       "gender_id            0\n",
       "country_id           0\n",
       "bought_product       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_travelers_p.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_travelers_p.dropna(axis=0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aa0bb7f"
   },
   "source": [
    "## TESTING FUNCTIONS SET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "45249731"
   },
   "outputs": [],
   "source": [
    "# Init params \n",
    "\n",
    "# model = RandomForestRegressor()\n",
    "# classification = False\n",
    "# evaluation_metric = 'neg_mean_squared_error'\n",
    "# dataframe = reg_df.copy()\n",
    "# target_label = 'clicks'\n",
    "# date_cols = ['departure_date'] \n",
    "\n",
    "model = RandomForestClassifier()\n",
    "classification = True\n",
    "evaluation_metric= 'accuracy'\n",
    "#dataframe = clf_df\n",
    "target_label = 'bought_product'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb8cdee4",
    "outputId": "8b2c08fb-3660-4c35-c22e-e20216ab6fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1\n",
      "Converting columns to lowercase\n",
      "Ran 3 checks on the dataframe\n",
      "All the checks were passed..\n",
      "Your dataframe seems to be correct. We return the original input data\n",
      "Converting columns to lowercase\n",
      "There are missing values in your dataset\n",
      "Ran 3 checks on the dataframe\n",
      "Sanity checks FAILED: (failures=1)\n",
      "Converting to int float and dates\n",
      "Column being processed: mosaic\n",
      "Column being processed: avg_leadtime\n",
      "Convert to float avg_leadtime\n",
      "Column being processed: averageflightduration_imputedvalue\n",
      "Convert to float averageflightduration_imputedvalue\n",
      "Column being processed: averagepricepaid\n",
      "Convert to float averagepricepaid\n",
      "Column being processed: busyness\n",
      "Convert to float busyness\n",
      "Column being processed: basicholidaycostperpax_imputedvalue\n",
      "Convert to float basicholidaycostperpax_imputedvalue\n",
      "Column being processed: averagespendperactiveyear\n",
      "Convert to float averagespendperactiveyear\n",
      "Column being processed: pricesensitivity\n",
      "Convert to float pricesensitivity\n",
      "Column being processed: totalrevenue\n",
      "Convert to float totalrevenue\n",
      "Column being processed: pricedifference\n",
      "Convert to float pricedifference\n",
      "Column being processed: tratingnew\n",
      "Convert to float tratingnew\n",
      "Column being processed: correctedholidayduration_imputedvalue\n",
      "Convert to int correctedholidayduration_imputedvalue\n",
      "Column being processed: frequency\n",
      "Convert to float frequency\n",
      "Column being processed: activeperiod\n",
      "Convert to float activeperiod\n",
      "Column being processed: household_income___median_income\n",
      "Convert to int household_income___median_income\n",
      "Column being processed: affluence\n",
      "Convert to int affluence\n",
      "Column being processed: urbanity___generalised_urbanity_measure\n",
      "Convert to float urbanity___generalised_urbanity_measure\n",
      "Column being processed: rurality___remoteness_from_high_streets\n",
      "Convert to float rurality___remoteness_from_high_streets\n",
      "Column being processed: sum_isweb\n",
      "Convert to int sum_isweb\n",
      "Column being processed: boardscore\n",
      "Convert to float boardscore\n",
      "Column being processed: countryname.spain\n",
      "Convert to float countryname.spain\n",
      "Column being processed: boardbasisoffersdescription.all.inclusive\n",
      "Convert to float boardbasisoffersdescription.all.inclusive\n",
      "Column being processed: haultype.mid\n",
      "Convert to float haultype.mid\n",
      "Column being processed: destinationloyalty\n",
      "Convert to float destinationloyalty\n",
      "Column being processed: sum_isretail\n",
      "Convert to int sum_isretail\n",
      "Column being processed: brandloyalty\n",
      "Convert to float brandloyalty\n",
      "Column being processed: haultype.short\n",
      "Convert to float haultype.short\n",
      "Column being processed: haulloyalty\n",
      "Convert to float haulloyalty\n",
      "Column being processed: avg_issummer\n",
      "Convert to float avg_issummer\n",
      "Column being processed: boardbasisoffersdescription.half.board\n",
      "Convert to float boardbasisoffersdescription.half.board\n",
      "Column being processed: months\n",
      "Convert to int months\n",
      "Column being processed: segment\n"
     ]
    }
   ],
   "source": [
    "#The following function is not is use\n",
    "\n",
    "# Skip this function \n",
    "\n",
    "# print('Part 0')\n",
    "# key_path = '/Users/carlosdelacruz/Desktop/main_folders/passwords_keys/keywords.txt'\n",
    "# base = '/Users/carlosdelacruz/Desktop/main_folders/DSProjects/AutopilotProject/tuiautopilotml_notebooks/sql_files/'\n",
    "# sql_file_location = base + 'needs_oct_dec.sql'\n",
    "# extract_from_database_to_df(sql_file_location = sql_file_location, mode ='from_database', \n",
    "#                                  new_file_title ='traveller_features_oct_dec',\n",
    "#                                  type_of_connection='snowflake', save_to_csv = True, \n",
    "#                                  sf_password_file_location=key_path, sf_user_name='CARLOS_DELACRUZ', \n",
    "#                                  sf_account='tuinordic.eu-central-1',sf_role='SYSADMIN')\n",
    "\n",
    "print('Part 1')\n",
    "transformed_df = dataframe_transformation(dataframe, cols_to_exclude=None,  drop_missing_values=False)                       \n",
    "transformed_df_with_missing = dataframe_transformation(clf_df_with_missing, cols_to_exclude=None,  drop_missing_values=False)                       \n",
    "\n",
    "t1 = shuffle_order_save(dataframe, shuffle=False, sample_size=100, save_sample_df=False )\n",
    "t2 = shuffle_order_save(dataframe, shuffle=True, sample_size=100, save_sample_df=False )\n",
    "\n",
    "print('Part 2')\n",
    "n_estimators = [100, 200]\n",
    "xgb_param_grid = dict( n_estimators=n_estimators )\n",
    "func = get_cross_val_score_wrapper\n",
    "func2 = grid_search_wrapper\n",
    "model_training_estimator_wrapper(func= func2, dataframe= transformed_df[:700], split_pct_param=0.01, patience_limit=0.2, target_label = target_label, param_grid=xgb_param_grid ,model = model, \n",
    "                                 evaluation_metric=\"accuracy\", n_jobs=-1, verbose = 3, n_folds=3, n_repeats=3,\n",
    "                                 k_fold_method='stratified_k_fold', grid_search_method='randomized', random_state=seed)\n",
    "                        \n",
    "\n",
    "print('Part 3')\n",
    "\n",
    "\n",
    "initial_eda_wrapper(dataframe=transformed_df, target_label = target_label, summary_report = False,\n",
    "                         return_outliers = False , distribution = 'non_gaussian' , \n",
    "                     save_figures = False)\n",
    "\n",
    "initial_eda_wrapper(dataframe=transformed_df, target_label = target_label, summary_report = False, \n",
    "                        return_outliers = True , distribution = 'non_gaussian' , \n",
    "                     save_figures = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a146fa0a"
   },
   "source": [
    "## TESTING FUNCTIONS SET 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57d997db",
    "outputId": "6eb727d6-8bd3-4b89-b7c7-69d2f0e515b7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 6\n",
      "Label encode and one hot encode only objects\n"
     ]
    }
   ],
   "source": [
    " print('Part 4')\n",
    "X,y = get_splits_wrapper(dataframe = transformed_df, target_label =target_label , train_split=False, scaled=False, scaler=StandardScaler(),\n",
    "                     validation_set=False , test_size = 0.2)\n",
    "\n",
    "\n",
    "print('Part 5')\n",
    "imputation_df = eval_imputation_method_wrapper(dataframe= transformed_df_with_missing, target_label= target_label, model=model, \n",
    "                                     classification=classification, evaluation_metric=evaluation_metric)\n",
    "\n",
    "\n",
    "print('Part 6')\n",
    "samp_df_encoded = get_encoded_wrapper(transformed_df) \n",
    "\n",
    "print(samp_df_encoded.shape)\n",
    "samp_df_encoded2, map_ = get_encoded_wrapper(transformed_df, return_mapping =True) \n",
    "nulls_encoded,map_ = get_encoded_wrapper(transformed_df_with_missing, encode_nulls=True, return_mapping=True)\n",
    "\n",
    "\n",
    "print('Part 7')\n",
    "#OBS: XGBOOST WILL NOT RETURN VALUES WITH A SMALL DATASET , EX: 500\n",
    "  \n",
    "scalers_scores1, output_df1 = eval_model_scaler_wrapper(dataframe=samp_df_encoded[:1000], target_label=target_label, model_name='KNN', k_fold_method='k_fold', n_folds=3,\n",
    "                             n_repeats=10,\n",
    "                             classification=classification,\n",
    "                             evaluation_metric=evaluation_metric)\n",
    "\n",
    "print('Part 7.1')\n",
    "scalers_scores2, output_df2= eval_model_scaler_wrapper(dataframe=samp_df_encoded[:1000], target_label=target_label, model_name='KNN', k_fold_method='repeated_k_fold', n_folds=3,\n",
    "                             n_repeats=3,\n",
    "                             classification=classification,\n",
    "                             evaluation_metric=evaluation_metric)\n",
    "\n",
    "\n",
    "print('Part 8')\n",
    "outliers_scores1,outliers_df1 = handle_outliers(dataframe = samp_df_encoded[:1000], target_label=target_label, tot_outlier_pct=15, model= RandomForestClassifier(),\n",
    "                         test_size=0.2)\n",
    "\n",
    "outliers_scores2,outliers_df2  = handle_outliers(dataframe = samp_df_encoded[:1000],distribution=None, target_label=target_label, tot_outlier_pct=5, model= RandomForestClassifier(),\n",
    "                         test_size=0.2)\n",
    "\n",
    "\n",
    "print('Part 9')\n",
    "feature_importance1 = get_feature_importance_wrapper(dataframe=samp_df_encoded , target_label=target_label ,method = 'rf' , classification = classification,\n",
    "                                        n_features = 5 \n",
    "                            , save_figure = False )\n",
    "\n",
    "feature_importance2 = get_feature_importance_wrapper(dataframe=samp_df_encoded , target_label=target_label ,method = 'xgb' , classification = classification, n_features = 2 ,\n",
    "                                  penalty = \"l2\" ,  save_figure = False )\n",
    "\n",
    "print('Part 10')    \n",
    "\n",
    "oversamplers1 = evaluate_oversamplers(dataframe=samp_df_encoded, target_label=target_label,classification=True, evaluation_metric='accuracy',  test_size=0.2, method='random_os', \n",
    "                      class_threshold=5,\n",
    "                          model=model,random_state=0 )\n",
    "\n",
    "oversamplers2 = evaluate_oversamplers(dataframe=samp_df_encoded, target_label=target_label, test_size=0.2, method='smote_os', \n",
    "                      class_threshold=5,model=model,random_state=0 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "740706f1"
   },
   "source": [
    "## TESTING FUNCTIONS SET 3 (train vs test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae86b3fa",
    "outputId": "2b7a2e4e-1169-40ec-efde-5b262068a458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part train vs test \n",
      "Current folds: [0, 2000, 4000, 6000, 8000]\n",
      "Label encode and one hot encode only objects\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for fold 0: [0.4860957850953448, 0.006786581004938409]\n",
      "Label encode and one hot encode only objects\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for fold 1: [0.4927663669366858, 0.013370698625128914]\n",
      "Label encode and one hot encode only objects\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for fold 2: [0.5056172095916173, 0.022639800438454837]\n",
      "Label encode and one hot encode only objects\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for fold 3: [0.46903843571049153, 0.012984277164318402]\n",
      "Mean score: 0.48837944933353483, Standard deviation: 0.01318782919107137\n",
      "Current folds: [0, 2000, 4000, 6000, 8000]\n",
      "Train shape(2000, 32)\n",
      "Label encode and one hot encode only objects\n",
      "Col:Mosaic\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Mosaic in fold 0: [0.49672379054226035, 0.015717375552562345]\n",
      "Col:Avg_LeadTime\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Avg_LeadTime in fold 0: [0.49467314211593766, 0.015227734234843329]\n",
      "Col:AverageFlightDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AverageFlightDuration_ImputedValue in fold 0: [0.4982993999047567, 0.015160377155462515]\n",
      "Col:AveragePricePaid\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AveragePricePaid in fold 0: [0.5280214619590358, 0.013432768835110277]\n",
      "Col:Busyness\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Busyness in fold 0: [0.5118730713774684, 0.011260160091884613]\n",
      "Col:BasicHolidayCostperPAX_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BasicHolidayCostperPAX_ImputedValue in fold 0: [0.49385029845574796, 0.00726167939512583]\n",
      "Col:AverageSpendPerActiveYear\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AverageSpendPerActiveYear in fold 0: [0.4740385937470258, 0.010214672249031215]\n",
      "Col:PriceSensitivity\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceSensitivity in fold 0: [0.48300682223193975, 0.014305714506523636]\n",
      "Col:TotalRevenue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TotalRevenue in fold 0: [0.4893365613240631, 0.014664878639367008]\n",
      "Col:PriceDifference\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceDifference in fold 0: [0.48549015222656855, 0.017102424610684175]\n",
      "Col:TRatingNew\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TRatingNew in fold 0: [0.5167458038520695, 0.009668069447908502]\n",
      "Col:CorrectedHolidayDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for CorrectedHolidayDuration_ImputedValue in fold 0: [0.4882291778910286, 0.00590611133664147]\n",
      "Col:Frequency\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Frequency in fold 0: [0.4894445737414489, 0.017704957909871154]\n",
      "Col:ActivePeriod\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for ActivePeriod in fold 0: [0.4976484087684206, 0.013361805847365137]\n",
      "Col:Household_income___Median_income\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Household_income___Median_income in fold 0: [0.48976942874384405, 0.022353501344267564]\n",
      "Col:Affluence\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Affluence in fold 0: [0.4958601479213396, 0.020614077844084755]\n",
      "Col:Urbanity___Generalised_urbanity_measure\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Urbanity___Generalised_urbanity_measure in fold 0: [0.5039501161418539, 0.013569185042887082]\n",
      "Col:Rurality___Remoteness_from_high_streets\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Rurality___Remoteness_from_high_streets in fold 0: [0.49275230658055164, 0.011563718808323274]\n",
      "Col:Sum_IsWeb\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Sum_IsWeb in fold 0: [0.49904686517766617, 0.017056174122732266]\n",
      "Col:BoardScore\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardScore in fold 0: [0.5038485144900343, 0.01120304996470764]\n",
      "Col:CountryName.Spain\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for CountryName.Spain in fold 0: [0.5017193728054015, 0.0064397803782800835]\n",
      "Col:BoardBasisOffersDescription.All.Inclusive\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardBasisOffersDescription.All.Inclusive in fold 0: [0.5023222575573506, 0.014460484444920712]\n",
      "Col:HaulType.Mid\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulType.Mid in fold 0: [0.5162285449461796, 0.01373010655151887]\n",
      "Col:DestinationLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for DestinationLoyalty in fold 0: [0.49942350326436447, 0.016946299737868327]\n",
      "Col:Sum_IsRetail\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Sum_IsRetail in fold 0: [0.5122996861396472, 0.012155592585854864]\n",
      "Col:BrandLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BrandLoyalty in fold 0: [0.4892176868430978, 0.010047444867808326]\n",
      "Col:HaulType.Short\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulType.Short in fold 0: [0.48944539831698997, 0.0030978158708732995]\n",
      "Col:HaulLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulLoyalty in fold 0: [0.4890277206032983, 0.00864756565065973]\n",
      "Col:Avg_IsSummer\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Avg_IsSummer in fold 0: [0.5078402763610101, 0.009525736830021959]\n",
      "Col:BoardBasisOffersDescription.Half.Board\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for BoardBasisOffersDescription.Half.Board in fold 0: [0.49421725183072046, 0.012836099364386611]\n",
      "Col:Months\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Months in fold 0: [0.47167023415593007, 0.013836078133616701]\n",
      "Col:Segment\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Segment in fold 0: [0.4896689052998336, 0.018527226231838657]\n",
      "Train shape(2000, 32)\n",
      "Label encode and one hot encode only objects\n",
      "Col:Mosaic\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Mosaic in fold 1: [0.5043781599011338, 0.016247092177376937]\n",
      "Col:Avg_LeadTime\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Avg_LeadTime in fold 1: [0.5043491362655024, 0.025122983776035127]\n",
      "Col:AverageFlightDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AverageFlightDuration_ImputedValue in fold 1: [0.5071076424804191, 0.016621177454794158]\n",
      "Col:AveragePricePaid\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AveragePricePaid in fold 1: [0.4973193192817719, 0.013727439460330879]\n",
      "Col:Busyness\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Busyness in fold 1: [0.48748982638022265, 0.019258026037754467]\n",
      "Col:BasicHolidayCostperPAX_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BasicHolidayCostperPAX_ImputedValue in fold 1: [0.48682823959947485, 0.002981536323494695]\n",
      "Col:AverageSpendPerActiveYear\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AverageSpendPerActiveYear in fold 1: [0.4814025368987938, 0.0224603278336136]\n",
      "Col:PriceSensitivity\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceSensitivity in fold 1: [0.49963271052707636, 0.016022422489948195]\n",
      "Col:TotalRevenue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TotalRevenue in fold 1: [0.5032834254133978, 0.018714172015495037]\n",
      "Col:PriceDifference\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceDifference in fold 1: [0.49315004332487866, 0.01982491109180384]\n",
      "Col:TRatingNew\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TRatingNew in fold 1: [0.4944827068125, 0.00927994676316755]\n",
      "Col:CorrectedHolidayDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for CorrectedHolidayDuration_ImputedValue in fold 1: [0.49913320603411454, 0.014169338931254293]\n",
      "Col:Frequency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Frequency in fold 1: [0.5054091115825287, 0.02096775377395564]\n",
      "Col:ActivePeriod\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for ActivePeriod in fold 1: [0.503380920586387, 0.012093555484924159]\n",
      "Col:Household_income___Median_income\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Household_income___Median_income in fold 1: [0.4759941051979205, 0.01211585165175024]\n",
      "Col:Affluence\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Affluence in fold 1: [0.47679191762425627, 0.014677841894299071]\n",
      "Col:Urbanity___Generalised_urbanity_measure\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Urbanity___Generalised_urbanity_measure in fold 1: [0.495721903841018, 0.007755649073666108]\n",
      "Col:Rurality___Remoteness_from_high_streets\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Rurality___Remoteness_from_high_streets in fold 1: [0.4916254857808779, 0.012844510756118456]\n",
      "Col:Sum_IsWeb\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Sum_IsWeb in fold 1: [0.49559069594413074, 0.012357195171733386]\n",
      "Col:BoardScore\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardScore in fold 1: [0.5039669266266792, 0.021164721533642388]\n",
      "Col:CountryName.Spain\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for CountryName.Spain in fold 1: [0.5007205332520441, 0.008346307006288318]\n",
      "Col:BoardBasisOffersDescription.All.Inclusive\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardBasisOffersDescription.All.Inclusive in fold 1: [0.5070699225650585, 0.008936482076806062]\n",
      "Col:HaulType.Mid\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulType.Mid in fold 1: [0.5194036943246807, 0.017192374655305705]\n",
      "Col:DestinationLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for DestinationLoyalty in fold 1: [0.508484444334977, 0.01239633368322961]\n",
      "Col:Sum_IsRetail\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Sum_IsRetail in fold 1: [0.49925794009734303, 0.015147167673777613]\n",
      "Col:BrandLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BrandLoyalty in fold 1: [0.4932433670789256, 0.0205377497566369]\n",
      "Col:HaulType.Short\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulType.Short in fold 1: [0.497919566790345, 0.014271163293882352]\n",
      "Col:HaulLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulLoyalty in fold 1: [0.4890490749605285, 0.012724312186098685]\n",
      "Col:Avg_IsSummer\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Avg_IsSummer in fold 1: [0.49233403750256144, 0.008822735803833028]\n",
      "Col:BoardBasisOffersDescription.Half.Board\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardBasisOffersDescription.Half.Board in fold 1: [0.48322953373985317, 0.019136564163467675]\n",
      "Col:Months\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Months in fold 1: [0.5024767408987946, 0.009991182509752543]\n",
      "Col:Segment\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Segment in fold 1: [0.5217663290424225, 0.02199286603660534]\n",
      "Train shape(2000, 32)\n",
      "Label encode and one hot encode only objects\n",
      "Col:Mosaic\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Mosaic in fold 2: [0.49955681296708915, 0.01853737513481034]\n",
      "Col:Avg_LeadTime\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Avg_LeadTime in fold 2: [0.5107589010590361, 0.03134633158906407]\n",
      "Col:AverageFlightDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for AverageFlightDuration_ImputedValue in fold 2: [0.5098411935500785, 0.014157868401165176]\n",
      "Col:AveragePricePaid\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for AveragePricePaid in fold 2: [0.49585351644922326, 0.017517165686830843]\n",
      "Col:Busyness\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Busyness in fold 2: [0.5014396668798972, 0.01782210672544079]\n",
      "Col:BasicHolidayCostperPAX_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.04 minutes to run\n",
      "Score for BasicHolidayCostperPAX_ImputedValue in fold 2: [0.5067271425200147, 0.0048291060039563245]\n",
      "Col:AverageSpendPerActiveYear\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for AverageSpendPerActiveYear in fold 2: [0.5053164614359394, 0.010662467276273298]\n",
      "Col:PriceSensitivity\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceSensitivity in fold 2: [0.5026647870905943, 0.011253080749817393]\n",
      "Col:TotalRevenue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TotalRevenue in fold 2: [0.5165700820407834, 0.018243221755997113]\n",
      "Col:PriceDifference\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceDifference in fold 2: [0.48873984051321795, 0.014412366205639474]\n",
      "Col:TRatingNew\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TRatingNew in fold 2: [0.49169706234226335, 0.01260844149958011]\n",
      "Col:CorrectedHolidayDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for CorrectedHolidayDuration_ImputedValue in fold 2: [0.5059416705429317, 0.01860394334893537]\n",
      "Col:Frequency\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Frequency in fold 2: [0.4935032570765773, 0.022533091959907325]\n",
      "Col:ActivePeriod\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for ActivePeriod in fold 2: [0.4851936037059003, 0.019199616270484464]\n",
      "Col:Household_income___Median_income\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Household_income___Median_income in fold 2: [0.5007559162732609, 0.016364624853916362]\n",
      "Col:Affluence\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Affluence in fold 2: [0.4979934138056706, 0.016317820482186652]\n",
      "Col:Urbanity___Generalised_urbanity_measure\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Urbanity___Generalised_urbanity_measure in fold 2: [0.5216568053899419, 0.01336349204868009]\n",
      "Col:Rurality___Remoteness_from_high_streets\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Rurality___Remoteness_from_high_streets in fold 2: [0.5155485164228243, 0.014474517175086176]\n",
      "Col:Sum_IsWeb\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Sum_IsWeb in fold 2: [0.5076200874866076, 0.030665694408377575]\n",
      "Col:BoardScore\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardScore in fold 2: [0.5007178623611753, 0.018799404156957814]\n",
      "Col:CountryName.Spain\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for CountryName.Spain in fold 2: [0.5166280779582575, 0.01996653877948499]\n",
      "Col:BoardBasisOffersDescription.All.Inclusive\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BoardBasisOffersDescription.All.Inclusive in fold 2: [0.5013494661127197, 0.01488119240294617]\n",
      "Col:HaulType.Mid\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulType.Mid in fold 2: [0.5040063393501283, 0.014155624104363603]\n",
      "Col:DestinationLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for DestinationLoyalty in fold 2: [0.5098156721944223, 0.010074167414963663]\n",
      "Col:Sum_IsRetail\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Sum_IsRetail in fold 2: [0.5177483967685697, 0.020125402413986873]\n",
      "Col:BrandLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BrandLoyalty in fold 2: [0.5010792992283296, 0.012989972729411174]\n",
      "Col:HaulType.Short\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for HaulType.Short in fold 2: [0.5095038279410187, 0.020477988801001133]\n",
      "Col:HaulLoyalty\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for HaulLoyalty in fold 2: [0.49950898849555436, 0.01552165049950286]\n",
      "Col:Avg_IsSummer\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Avg_IsSummer in fold 2: [0.5203333922260038, 0.013878751283598095]\n",
      "Col:BoardBasisOffersDescription.Half.Board\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for BoardBasisOffersDescription.Half.Board in fold 2: [0.4926968715925117, 0.004584303422580939]\n",
      "Col:Months\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Months in fold 2: [0.4743523883768316, 0.011831425159826024]\n",
      "Col:Segment\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Segment in fold 2: [0.5008484939932191, 0.019470049653943965]\n",
      "Train shape(2000, 32)\n",
      "Label encode and one hot encode only objects\n",
      "Col:Mosaic\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for Mosaic in fold 3: [0.47066068273162626, 0.011780649762893293]\n",
      "Col:Avg_LeadTime\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Avg_LeadTime in fold 3: [0.502976844434671, 0.022988802906357235]\n",
      "Col:AverageFlightDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for AverageFlightDuration_ImputedValue in fold 3: [0.4884657415487933, 0.01547462301303252]\n",
      "Col:AveragePricePaid\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for AveragePricePaid in fold 3: [0.5054022038287213, 0.008062643432194514]\n",
      "Col:Busyness\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for Busyness in fold 3: [0.5033778873075594, 0.02250052050411689]\n",
      "Col:BasicHolidayCostperPAX_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for BasicHolidayCostperPAX_ImputedValue in fold 3: [0.4912889933631873, 0.025703778386941056]\n",
      "Col:AverageSpendPerActiveYear\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for AverageSpendPerActiveYear in fold 3: [0.5058393508794393, 0.016575275095858072]\n",
      "Col:PriceSensitivity\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceSensitivity in fold 3: [0.48064045616697737, 0.00855205180613289]\n",
      "Col:TotalRevenue\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for TotalRevenue in fold 3: [0.5050928502945865, 0.013099619042965902]\n",
      "Col:PriceDifference\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for PriceDifference in fold 3: [0.49695369901061676, 0.023468024618334693]\n",
      "Col:TRatingNew\n",
      "Collecting garbage...\n",
      "The process took: 0.02 minutes to run\n",
      "Score for TRatingNew in fold 3: [0.48818267641032326, 0.01575774842777205]\n",
      "Col:CorrectedHolidayDuration_ImputedValue\n",
      "Collecting garbage...\n",
      "The process took: 0.03 minutes to run\n",
      "Score for CorrectedHolidayDuration_ImputedValue in fold 3: [0.4957202719618016, 0.015791026812921664]\n",
      "Col:Frequency\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/by/pt5_hysn0lb75x63vpgqbtsw0000gn/T/ipykernel_71200/1060063804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# test_comp.train_test_pairplot()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmean_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_comp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_covariance_shift_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Segment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcov_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_comp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_covariance_shift_score_per_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_score_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/main_folders/DSProjects/AutopilotProject/tuiautopilotml/tuiautopilotml/wrappers.py\u001b[0m in \u001b[0;36mget_covariance_shift_score_per_feature\u001b[0;34m(self, estimator, cov_score_thresh, n_folds, n_repeats, random_state)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                                      \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                                      \u001b[0mclassification\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                                      evaluation_metric='roc_auc')\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Score for {col} in fold {fold}: {scores}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/main_folders/DSProjects/AutopilotProject/tuiautopilotml/tuiautopilotml/helper_functions.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcurrent_time_minutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/main_folders/DSProjects/AutopilotProject/tuiautopilotml/tuiautopilotml/helper_functions.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Collecting garbage...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/main_folders/DSProjects/AutopilotProject/tuiautopilotml/tuiautopilotml/helper_functions.py\u001b[0m in \u001b[0;36mget_cross_val_score_wrapper\u001b[0;34m(dataframe, target_label, x, y, model, multiple_eval_scores, k_fold_method, n_folds, n_repeats, random_state, classification, multi_classif, evaluation_metric, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0mvalidator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossValidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0mcv_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cross_validation_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# To ensure this function continues to do what it originally did, we take the means here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/main_folders/DSProjects/AutopilotProject/tuiautopilotml/tuiautopilotml/cross_validation.py\u001b[0m in \u001b[0;36mget_cross_validation_scores\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEvalMetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             scores = ms.cross_val_score(self.model, self.dataset.inputs, self.dataset.labels, cv=self.policy,\n\u001b[0;32m--> 301\u001b[0;31m                                         scoring=metric.value, n_jobs=self.config.n_jobs, verbose=self.config.verbose)\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCrossValidationResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    448\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    451\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 256\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# For callabe scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/autopilot_env/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Part train vs test ')\n",
    "\n",
    "train, test = train_test_split_from_df(dataframe, 0.2)\n",
    "test_comp = TrainVsTest(train,test)\n",
    "test_comp.get_report('Segment')\n",
    "test_comp.get_train_test_distribution()\n",
    "test_comp.get_train_test_counts()\n",
    "a, b = test_comp.is_distribution_different()\n",
    "full_data = test_comp.get_is_train_col()\n",
    "test_comp.train_test_pairplot()\n",
    "mean_score, std  = test_comp.get_covariance_shift_score(target_label = 'Segment')\n",
    "cov_scores, drop_list = test_comp.get_covariance_shift_score_per_feature(cov_score_thresh=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "583919ab"
   },
   "source": [
    "## TESTING FUNCTIONS SET 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d4e7c81",
    "outputId": "0f06200b-d4a0-4c9c-decb-f7378a10e7c9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 15\n",
      "Model name: XGB, Parameters: {'n_estimators': <hyperopt.pyll.base.Apply object at 0x7f8b080a1c50>, 'max_depth': <hyperopt.pyll.base.Apply object at 0x7f8b080a1f90>, 'min_child_weight': <hyperopt.pyll.base.Apply object at 0x7f8b080a91d0>, 'gamma': <hyperopt.pyll.base.Apply object at 0x7f8b080a9490>}\n",
      "[07:47:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[07:47:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[07:47:25] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Collecting garbage...                                                           \n",
      "The process took: 0.3 minutes to run                                            \n",
      "Accuracy: 0.625020229810649                                                     \n",
      "  1%|         | 1/80 [00:18<23:43, 18.02s/trial, best loss: -0.625020229810649]\n",
      "optimization complete\n",
      "Results:{'XGB': (0.625020229810649, 0.019659251376223486)}\n"
     ]
    }
   ],
   "source": [
    "print('Part 13')\n",
    "\n",
    "multi_scores_df = evaluate_models_wrapper(dataframe=samp_df_encoded[:1000], target_label='segment', models_list=models_list_default, classification=True, multiple_eval_scores=True, stacking=True)\n",
    "\n",
    "evaluate_models_wrapper(dataframe=samp_df_encoded[:1000], target_label='segment', models_list=models_list_default, classification=True, multiple_eval_scores=False, stacking=True)\n",
    "\n",
    "\n",
    "print('Part 13.1 Reduced features score')\n",
    "\n",
    "scores_redcued_features = get_reduced_features_cv_scores(samp_df_encoded, target_label, model)\n",
    "\n",
    "\n",
    "print('Part 14')\n",
    "\n",
    "n_estimators = [100, 200, 300, 400, 500]\n",
    "\n",
    "xgb_param_grid = dict(n_estimators=n_estimators)\n",
    "            \n",
    "\n",
    "X_scaled,y = get_splits_wrapper(dataframe = samp_df_encoded, target_label =target_label , train_split=False, scaled=True, scaler=StandardScaler(),\n",
    "                      validation_set=False , test_size = 0.2)\n",
    "\n",
    "output_grid_s = grid_search_wrapper(dataframe=samp_df_encoded , target_label = target_label,model= model, param_grid = xgb_param_grid, \n",
    "                             evaluation_metric=evaluation_metric, n_jobs=-1, verbose = 3, n_folds=3, n_repeats=3,\n",
    "                        k_fold_method='stratified_k_fold', grid_search_method='randomized', random_state= seed)\n",
    "\n",
    "print('Part 15')   \n",
    "results_list = hyper_opt_manual(dataframe= samp_df_encoded[:1000], target_label= target_label, model_name='XGB', max_evals=80, k_fold_method='k_fold', n_folds=3,\n",
    "                    n_repeats=2, classification=True, evaluation_metric='accuracy', timeout_minutes=0.5, n_jobs=-1,\n",
    "                    verbose=0)\n",
    "\n",
    "print('Part 15.1')  \n",
    "func = hyper_opt_manual\n",
    "\n",
    "ml_flow_wrapper(func=func,model_name='test_model_name', evaluation_metric = 'accuracy',dataframe= samp_df_encoded[:1000], target_label= target_label, model_name='XGB', max_evals=80, k_fold_method='k_fold', n_folds=3,\n",
    "                    n_repeats=2, classification=True, evaluation_metric='accuracy', timeout_minutes=0.5, n_jobs=-1,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e642711a"
   },
   "source": [
    "## TESTING FUNCTIONS SET 4 (functions are not updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5542dd4d"
   },
   "outputs": [],
   "source": [
    "# # RUN THIS CODE FIRST \n",
    "\n",
    "# def mlp_hyperopt_wrapper(X_train, X_valid, y_train, y_valid, X_test, y_test, activation_f_type='multiclass' ,classification=True, \n",
    "#                          metrics ='accuracy',monitor='val_loss', mode='min',hl_activation='relu',verbose=3, \n",
    "#                          patience=3,max_evals=100 , timeout_minutes=10):\n",
    "    \n",
    "#     if y_train.nunique() != y_valid.nunique():\n",
    "#         print(f' y_train and y _valid should have the same size')\n",
    "\n",
    "#     if activation_f_type == 'classif':\n",
    "#         o_activation = 'sigmoid'\n",
    "#         loss = 'binary_crossentropy'\n",
    "\n",
    "#     elif activation_f_type == 'multiclass':\n",
    "#         o_activation = 'softmax'\n",
    "#         loss = 'categorical_crossentropy'\n",
    "\n",
    "#     elif activation_f_type == 'reg':\n",
    "#         o_activation = 'linear'  # relu \n",
    "#         loss = 'mean_squared_error'\n",
    "\n",
    "#     timeout = timeout_minutes * 60\n",
    "#     space = get_initial_dicts(mode='hyperparams', params_list='MLP', classification=classification)['MLP']\n",
    "\n",
    "\n",
    "#     def hyperparameters(space):\n",
    "#         input_n = space['input_n']\n",
    "#         output_n = int(y_train.nunique())\n",
    "#         n_neurons = int(np.sqrt(input_n * output_n)* input_n)\n",
    "#         print(input_n)\n",
    "#         print(n_neurons)\n",
    "#         print(space)\n",
    "\n",
    "#         model = Sequential()\n",
    "\n",
    "#         #model.add(Dense(units=space['units1'], input_dim=X_train.shape[1], activation=hl_activation))  # input layer\n",
    "#         model.add(Dense(units=n_neurons/2.5, input_dim=X_train.shape[1], activation=hl_activation))  # input layer\n",
    "\n",
    "#         model.add(Dropout(space['dropout1']))\n",
    "\n",
    "#         #model.add(Dense(units=space['units2'], activation=hl_activation))\n",
    "#         model.add(Dense(units=n_neurons/5.5, input_dim=X_train.shape[1], activation=hl_activation))  # input layer\n",
    "\n",
    "#         model.add(Dropout(space['dropout2']))\n",
    "\n",
    "#         model.add(Dense(int(y_train.nunique()), activation=o_activation))  # output layer\n",
    "\n",
    "#         model.compile(loss= loss, optimizer=space['optimizer'], metrics=[metrics])\n",
    "#         early_stop = EarlyStopping(monitor=monitor, mode=mode, verbose=verbose, patience=patience)\n",
    "\n",
    "#         print('Fit model...')\n",
    "#         # transform y \n",
    "#         y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "#         y_valid_cat = tf.keras.utils.to_categorical(y_valid)\n",
    "\n",
    "#         model.fit(X_train, y_train_cat, epochs=space['epochs'], verbose=verbose, callbacks=[early_stop],\n",
    "#                   batch_size=space['batch_size'], validation_data=(X_valid, y_valid_cat))\n",
    "\n",
    "#         print('Predict...')\n",
    "#         prediction = model.predict(X_test, batch_size=space['batch_size'], verbose=verbose)\n",
    "#         prediction = np.argmax(prediction, axis=1)\n",
    "#         score = accuracy_score(y_test, prediction)\n",
    "#         print('Accuracy:', score)\n",
    "\n",
    "#         return {'loss': -score, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "#     trials = Trials()\n",
    "\n",
    "#     best = fmin(fn=hyperparameters,\n",
    "#                 space=space,\n",
    "#                 algo=tpe.suggest,\n",
    "#                 max_evals=max_evals,\n",
    "#                 trials=trials,\n",
    "#                 timeout=timeout\n",
    "#                 )\n",
    "#     print('optimization complete')\n",
    "#     best_model = trials.results[np.argmin([r['loss'] for r in\n",
    "#                                            trials.results])]\n",
    "\n",
    "#     best_score = best_model['loss'] * -1 \n",
    "#     #params = best_model['model'].get_params()#[algorithm]\n",
    "\n",
    "#     return best_score,trials\n",
    "\n",
    "\n",
    "# def mlp_basemodel_wrapper(X_train, X_valid, y_train, y_valid, activation_f_type = 'classif' , optimizer = 'adam' ,\n",
    "#                           regulator = 10,\n",
    "#                           hl_activation = 'relu', epochs = 100 , batch_size = 128 ,metrics = 'accuracy',\n",
    "#                           metric_to_monitor = 'val_loss', mode = 'minimize', patience = 10, verbose = 1  ):\n",
    "#     \"\"\"\n",
    "#     MLP for baseline model creation \n",
    "#     \"\"\"\n",
    "#     print(f'Numeber of features: {X_train.shape[1]}')\n",
    "\n",
    "#     input_n = X_train.shape[1]\n",
    "#     output_n = int(y_train.nunique())\n",
    "    \n",
    "#     if activation_f_type == 'classif':\n",
    "#         o_activation = 'sigmoid'\n",
    "#         loss='binary_crossentropy'\n",
    "    \n",
    "#     elif  activation_f_type == 'multiclass':\n",
    "#         o_activation = 'softmax'\n",
    "#         loss='categorical_crossentropy'\n",
    "    \n",
    "#     elif  activation_f_type == 'reg':\n",
    "#         o_activation = 'linear'# relu \n",
    "#         loss='mean_squared_error'\n",
    "    \n",
    "#     print(f'Activation function used for output layer: {o_activation}')\n",
    "    \n",
    "#     n_neurons = int(np.sqrt(input_n * output_n)* regulator)\n",
    "#     print(f'Number of neurons: {n_neurons}-{int(n_neurons/2.5)}-{int(n_neurons/5.5)}')\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(n_neurons, input_dim = input_n, activation= hl_activation))# input layer \n",
    "\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Dense(int(n_neurons/2.5), activation= hl_activation))\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Dense(int(n_neurons/5.5) , activation=hl_activation))\n",
    "#     model.add(Dropout(0.1))\n",
    "    \n",
    "#     model.add(Dense(output_n, activation= o_activation))# output layer \n",
    "    \n",
    "    \n",
    "#     model.compile(loss=loss, optimizer = optimizer, metrics=[metrics])\n",
    "#     early_stop = EarlyStopping(monitor = metric_to_monitor, mode = mode, verbose = verbose, patience = patience)\n",
    "#     print('Fit model...')\n",
    "    \n",
    "#     print(f'converting y{y_train.shape}')\n",
    "#     y_valid = tf.keras.utils.to_categorical(y_valid)\n",
    "#     y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    \n",
    "#     print(f'converted y{y_train.shape}')\n",
    "#     model.fit(X_train, y_train, epochs = epochs , verbose = verbose , callbacks=[early_stop] , \n",
    "#               batch_size = batch_size , validation_data= (X_valid,y_valid ) )\n",
    "    \n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee38c003"
   },
   "outputs": [],
   "source": [
    "# print('Part 16') \n",
    "# X_train, X_valid, y_train, y_valid , X_test, y_test = get_splits_wrapper(dataframe = samp_df_encoded,\n",
    "#                                                                          target_label = target_label, train_split=True\n",
    "#                        ,validation_set=True , test_size = 0.2 , scaled=True , scaler= MinMaxScaler())\n",
    "\n",
    "# best_score, trials = mlp_hyperopt_wrapper(X_train, X_valid, y_train, y_valid , X_test, y_test ,hl_activation='relu', \n",
    "#                      activation_f_type='classif', patience= 1 ,max_evals =100, timeout_minutes = 2,\n",
    "#                                           monitor='val_accuracy', mode='max', )\n",
    "\n",
    "\n",
    "# print('Part 17')   \n",
    "# X, y  = get_splits_wrapper(dataframe = samp_df_encoded, target_label = 'segment', train_split=False\n",
    "#                       ,validation_set=False , test_size = 0.2 , scaled=True , scaler= MinMaxScaler())\n",
    "\n",
    "# best_params, best_score, study = optuna_wrapper(dataframe=samp_df_encoded,target_label=target_label,algorithm='XGB', \n",
    "#                                   params_list= ['n_estimators' ,'max_depth' , 'learning_rate'],\n",
    "#        n_minutes_limit= 1 , n_trials = 15  )\n",
    "\n",
    "\n",
    "# print('Part 18')   \n",
    "# neural_net_model = mlp_basemodel_wrapper(X_train, X_valid, y_train, y_valid, optimizer = 'adam' , \n",
    "#                                          hl_activation = 'relu', activation_f_type='multiclass' ,\n",
    "#                                          epochs = 200 , batch_size = 128, patience = 10, regulator= 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "929e2f07"
   },
   "source": [
    "# TESTING FUNCTIONS SECTION\n",
    "This sections does not have any particular structure. Its main goal is to use it to test functions in different orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ef592cb"
   },
   "source": [
    "## AUTOPILOT FUCNTIONS\n",
    "This section tests all functions related to the autopilot mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a,b = evaluate_oversamplers(dataframe=CONFIG_LOCAL['dataframe'], target_label=CONFIG_LOCAL['target_label'],classification=True, evaluation_metric='accuracy',  test_size=0.2, \n",
    "#                       class_threshold=5,\n",
    "#                           model=RandomForestClassifier(), random_state=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imputation_df = eval_imputation_method_wrapper(dataframe= CONFIG_LOCAL['dataframe'], target_label= target_label, model=RandomForestClassifier(), \n",
    "#                                      classification=classification, evaluation_metric=evaluation_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# a,b = handle_outliers(dataframe = get_encoded_wrapper(CONFIG_LOCAL['dataframe'].dropna()), target_label=target_label, tot_outlier_pct=15, model= RandomForestClassifier(),\n",
    "#                          test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# trans1 = dataframe_transformation(CONFIG_LOCAL['dataframe'])\n",
    "# scores, output_df = eval_model_scaler_wrapper(get_encoded_wrapper(trans1.dropna()),\n",
    "#                                               CONFIG_LOCAL['target_label'], CONFIG_LOCAL['model_name'],\n",
    "#                                               k_fold_method='k_fold', n_folds=5,\n",
    "#                                               n_repeats=10, classification=True, evaluation_metric='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# samp_df = get_encoded_wrapper(trips_travelers_p[:3000].dropna())\n",
    "\n",
    "# scores, best_model = evaluate_models_wrapper(dataframe=samp_df, target_label=target_label,\n",
    "#                                              models_list=models_list_default, classification=classification,\n",
    "#                                              multiple_eval_scores=False, stacking=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tasks load params of best model \n",
    "\n",
    "# def hyper_opt_manual(dataframe: pd.DataFrame, target_label: str, model_name=None, max_evals=80, k_fold_method='k_fold',\n",
    "#                      n_folds=3, n_repeats=2, classification=True, evaluation_metric='accuracy', timeout_minutes=10,\n",
    "#                      n_jobs=-1, verbose=0):\n",
    "#     \"\"\"\n",
    "\n",
    "#     Args:\n",
    "#         model_name:\n",
    "#         dataframe:\n",
    "#         target_label:\n",
    "#         max_evals:\n",
    "#         k_fold_method:\n",
    "#         n_folds:\n",
    "#         n_repeats:\n",
    "#         classification:\n",
    "#         evaluation_metric:\n",
    "#         timeout_minutes:\n",
    "#         n_jobs:\n",
    "#         verbose:\n",
    "\n",
    "#     Returns:\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     timeout = timeout_minutes * 60\n",
    "#     models_dict = models['clf'] if classification else models['reg']\n",
    "#     current_model_dict = select_custom_dict(models_dict, model_name)\n",
    "\n",
    "#     hyper_params_dict = hyper_params['clf'] if classification else hyper_params['reg']\n",
    "#     space = select_custom_dict(hyper_params_dict, model_name)[model_name]\n",
    "#     print(f'Model name: {model_name}, Parameters: {space}')\n",
    "\n",
    "#     scores = {}\n",
    "\n",
    "#     def objective(selected_space):\n",
    "#         # clone the current version of the model in order to avoid overwriting the dictionary\n",
    "#         model_base = clone(current_model_dict[model_name])\n",
    "\n",
    "#         model_base.set_params(**selected_space)\n",
    "\n",
    "#         score = get_cross_val_score_wrapper(dataframe=dataframe, target_label=target_label, model=model_base,\n",
    "#                                             classification=classification, evaluation_metric=evaluation_metric,\n",
    "#                                             k_fold_method=k_fold_method,\n",
    "#                                             n_folds=n_folds,\n",
    "#                                             n_repeats=n_repeats, n_jobs=-n_jobs, verbose=verbose)\n",
    "#         scores[model_name] = score[0]\n",
    "\n",
    "#         print(f'Accuracy: {score[0]}')\n",
    "\n",
    "#         # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "#         return {'loss': - score[0], 'std': score[1], 'status': STATUS_OK, 'model': model_base}\n",
    "\n",
    "#     trials = Trials()\n",
    "\n",
    "#     fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials,\n",
    "#          timeout=timeout)  # early_stop_fn = no_progress_loss(10)\n",
    "\n",
    "#     print('optimization complete')\n",
    "#     best_model = trials.results[np.argmin([r['loss'] for r in\n",
    "#                                            trials.results])]\n",
    "\n",
    "#     best_score = best_model['loss'] * -1\n",
    "#     std = best_model['std']\n",
    "\n",
    "#     params = {k: v for k, v in best_model['model'].get_params().items() if v is not None}\n",
    "\n",
    "#     results = {model_name: (best_score, std)}\n",
    "\n",
    "#     print(f'Results:{results}')\n",
    "\n",
    "#     return results, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "593f9aba"
   },
   "source": [
    "## Autopilot mode testing\n",
    "This section is only dedicated to the autopilot mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto generated nulls\n",
    "\n",
    "for i in range(0,len(trips_travelers_p),10):\n",
    "    trips_travelers_p['no_of_kids'].replace(i,np.nan, inplace=True)\n",
    "    \n",
    "trips_travelers_p.drop(['reservation_date','departure_date'],axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "baa0ae4a"
   },
   "outputs": [],
   "source": [
    "# IDEA: break this down into several dictionaries and combine into 1 \n",
    "\n",
    "CONFIG_LOCAL = {\n",
    "    'run_id_number': np.random.randint(1,100), \n",
    "    'dataframe': trips_travelers_p[:3000],#dataframe[:3000],\n",
    "    #'latest_df': pd.DataFrame(),\n",
    "    'cols_to_exclude': ['id', 'reservation_id', 'return_date', 'birthdate'], \n",
    "    'drop_missing_values': False,\n",
    "    'baseline_model_name': 'baseline_model_score',\n",
    "    'target_label': 'bought_product',\n",
    "    'scaled_df': True,\n",
    "    'models_list': models_list_default, # \n",
    "    #'model': RandomForestClassifier(random_state = seed),\n",
    "    'model_name': 'RF',\n",
    "    'scaler': scalers['Standard'],\n",
    "    'multi_classif': False,\n",
    "    'stacking': False,\n",
    "    'distribution': 'non_gaussian',\n",
    "    'tot_outlier_pct': 13,\n",
    "    'classification': True,\n",
    "    'evaluation_metric': 'accuracy',\n",
    "    'multiple_eval_scores': False,\n",
    "    'test_size': 0.2,\n",
    "    'k_fold_method': 'k_fold',\n",
    "    'n_folds': 5,\n",
    "    'n_repeats': 10,\n",
    "    'seed': 0,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': 0,\n",
    "    'max_evals': 80,\n",
    "    'timeout_minutes': 2, \n",
    "    'n_minutes_limit':0.5,\n",
    "    'class_threshold': 5, \n",
    "    'params_list':['n_estimators' ,'max_depth' , 'learning_rate']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc5e8f49",
    "outputId": "61dead51-0f46-4f8b-a875-34ed06edaddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current run_id: 49\n",
      "*********JOB:0: dataframe_transformation*********\n",
      "Dropping cols to exclude\n",
      "Converting columns to lowercase\n",
      "There are missing values in your dataset\n",
      "Ran 3 checks on the dataframe\n",
      "Sanity checks FAILED: (failures=1)\n",
      "Converting to int float and dates\n",
      "Updating config dataframe_transformation...\n",
      "*********JOB:1: handle_missing_values*********\n",
      "Training fold: 0\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.71]\n",
      "Training fold: 1\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.71, 0.6366666666666667]\n",
      "Training fold: 2\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.71, 0.6366666666666667, 0.615]\n",
      "Training fold: 3\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy median\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.71, 0.6366666666666667, 0.615, 0.6616666666666666]\n",
      "Training fold: 0\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6816666666666666]\n",
      "Training fold: 1\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6816666666666666, 0.6133333333333333]\n",
      "Training fold: 2\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6816666666666666, 0.6133333333333333, 0.6266666666666667]\n",
      "Training fold: 3\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy mean\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6816666666666666, 0.6133333333333333, 0.6266666666666667, 0.63]\n",
      "Training fold: 0\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.7]\n",
      "Training fold: 1\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.7, 0.6333333333333333]\n",
      "Training fold: 2\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.7, 0.6333333333333333, 0.625]\n",
      "Training fold: 3\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy most_frequent\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.7, 0.6333333333333333, 0.625, 0.6466666666666666]\n",
      "Training fold: 0\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6883333333333334]\n",
      "Training fold: 1\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6883333333333334, 0.6066666666666667]\n",
      "Training fold: 2\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6883333333333334, 0.6066666666666667, 0.6216666666666667]\n",
      "Training fold: 3\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Columns with missing values: ['no_of_kids']\n",
      "Running strategy constant\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.6883333333333334, 0.6066666666666667, 0.6216666666666667, 0.6366666666666667]\n",
      "drop_nulls_score...\n",
      "Training fold: 0\n",
      "Label encode and one hot encode only objects\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.639344262295082]\n",
      "Training fold: 1\n",
      "Label encode and one hot encode only objects\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.639344262295082, 0.6784140969162996]\n",
      "Training fold: 2\n",
      "Label encode and one hot encode only objects\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.639344262295082, 0.6784140969162996, 0.6652173913043479]\n",
      "Training fold: 3\n",
      "Label encode and one hot encode only objects\n",
      "Label encode and one hot encode only objects\n",
      "Custom cv scores:[0.639344262295082, 0.6784140969162996, 0.6652173913043479, 0.6465863453815262]\n",
      "Encoding nulls...\n",
      "Label encode and one hot encode only objects\n",
      "Training fold: 0\n",
      "Custom cv scores:[0.6283333333333333]\n",
      "Training fold: 1\n",
      "Custom cv scores:[0.6283333333333333, 0.6116666666666667]\n",
      "Training fold: 2\n",
      "Custom cv scores:[0.6283333333333333, 0.6116666666666667, 0.6066666666666667]\n",
      "Training fold: 3\n",
      "Custom cv scores:[0.6283333333333333, 0.6116666666666667, 0.6066666666666667, 0.6083333333333333]\n",
      "Results: {'get_imputed_x-median': (0.6558333333333333, 0.035365158623204884), 'get_imputed_x-mean': (0.6379166666666667, 0.026017488776248605), 'get_imputed_x-most_frequent': (0.65125, 0.029187492564833682), 'get_imputed_x-constant': (0.6383333333333333, 0.03075440347874322), 'drop_nulls-None': (0.6573905239743139, 0.015375697602947267), 'encoded_nulls_score-None': (0.61375, 0.008609990966571581)}\n",
      "***********Generating final output***********\n",
      "Current best method: drop_nulls-None\n",
      "Params to return:{'strategy': 'None'}\n",
      "returning params_excluded_dict\n",
      "Label encode and one hot encode only objects\n",
      "Updating config handle_missing_values...\n",
      "*********JOB:2: encoding*********\n",
      "Label encode and one hot encode only objects\n",
      "Updating config encoding...\n",
      "*********JOB:3: baseline_score*********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.4s remaining:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting garbage...\n",
      "The process took: 0.06 minutes to run\n",
      "Logging sklearn artifacts...\n",
      "Find your results here: http://localhost:5000/\n",
      "Score: 0.6460237989095023 Std:0.026695620030016288\n",
      "*********JOB:4: handle_outliers*********\n",
      "Mixed function\n",
      "Replace values\n",
      "mean\n",
      "Training fold: 0\n",
      "Custom cv scores:[0.6431535269709544]\n",
      "Training fold: 1\n"
     ]
    }
   ],
   "source": [
    "##### 'WORK IN PROGRESS' #####\n",
    "    \n",
    "default_steps = {'dataframe_transformation': dataframe_transformation,\n",
    "                'handle_missing_values': eval_imputation_method_wrapper, \n",
    "                 'encoding': get_encoded_wrapper,\n",
    "                 'baseline_score': get_baseline_score \n",
    "                 } \n",
    "\n",
    "modelling_steps = {'handle_outliers': handle_outliers, 'evaluate_oversamplers':evaluate_oversamplers, 'evaluate_models': evaluate_models_wrapper} \n",
    "\n",
    "post_modelling_steps = {'transformation_methods': eval_model_scaler_wrapper,'grid_search':grid_search_wrapper, 'hyper_param_opt': hyper_opt_manual,'optuna':optuna_wrapper} \n",
    "\n",
    "pipe_steps = {**default_steps, **modelling_steps, ** post_modelling_steps }  \n",
    "\n",
    "autopilot_mode(steps=pipe_steps , config_dict = CONFIG_LOCAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5fe8a2"
   },
   "source": [
    "# ************Apendix code************\n",
    "This section of the notebook contains experimental approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ca7cd34"
   },
   "outputs": [],
   "source": [
    "def get_cols_cardinality(dataframe,n_max_categories):\n",
    "\n",
    "    cols = [cname for cname in dataframe if dataframe[cname].nunique() <= n_max_categories and\n",
    "                            dataframe[cname].dtype == \"object\"]\n",
    "    return cols\n",
    "\n",
    "\n",
    "class Figures:\n",
    "\n",
    "    def __init__(self, input_data, target_label, asc=False, fig_title=f'Fig Title',\n",
    "                 x_title='X_axis_title', y_title='Y_axis_title', stage='current stage'):\n",
    "        \"\"\"Avalable stages: Exploratory Data Analysis ,Feature Engineering \"\"\"\n",
    "        self.input_data = input_data\n",
    "        self.target_label = target_label\n",
    "        self.asc = asc\n",
    "        self.fig_title = fig_title\n",
    "        self.x_title = x_title\n",
    "        self.y_title = y_title\n",
    "        self.stage = stage\n",
    "     \n",
    "  \n",
    "class OtherPLots(Figures):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "    def correlation_heatmap(self,figsize=(12, 7) ,palette=None , save_figure=False):\n",
    "        print('Correlation between variables')\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(self.input_data.corr(), cmap=palette, fmt='g', annot=False)\n",
    "        if save_figure:\n",
    "            save_figure_to_disk(main_folder=self.stage, figure_name= self.fig_title , save_as_plt = True)\n",
    "        plt.show()\n",
    "\n",
    "    def combination_plot(self,palette=None,save_figure=False ):\n",
    "        train, test = train_test_split_from_df(dataframe,test_size)\n",
    "\n",
    "        int_float_cols = self.input_data.select_dtypes([int, float]).columns\n",
    "        plt.figure(figsize=(10, (len(int_float_cols)) * 2 + 3))  # width , height\n",
    "\n",
    "        count = 1\n",
    "        for col in int_float_cols:\n",
    "            # Row 2\n",
    "            plt.subplot(len(int_float_cols), 2, count)  # n_rows , n columns , index\n",
    "            sns.boxplot(x=col, y=self.target_label, data=self.input_data, palette=palette)\n",
    "            count += 1\n",
    "\n",
    "            # Row 2\n",
    "            plt.subplot(len(int_float_cols), 2, count)\n",
    "            g = sns.kdeplot(self.input_data[col], palette=palette, alpha=0.6, shade=True)\n",
    "            g.set_xlabel(col)\n",
    "            # g.set_ylabel(\"Frequency\")\n",
    "            # g = g.legend([\"No Diesese\", \"Diesese\"])\n",
    "            count += 1\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_figure:\n",
    "            save_figure_to_disk(main_folder=self.stage, figure_name= self.fig_title , save_as_plt = True)\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cad9b52f"
   },
   "outputs": [],
   "source": [
    "class BarPlots(Figures):\n",
    "    \n",
    "    def order_keys_values(self):\n",
    "\n",
    "        if type(self.input_data) == dict:\n",
    "            self.input_data = dict(sorted(self.input_data.items(), key=lambda x: (x[1], x[0]), reverse=self.asc))\n",
    "            keys = list(self.input_data.keys())\n",
    "            values = list(self.input_data.values())\n",
    "            return keys, values\n",
    "\n",
    "        elif type(self.input_data) == pd.core.frame.DataFrame:\n",
    "            keys = list(self.input_data.index)\n",
    "            values = list(self.input_data.importance)\n",
    "            return keys, values\n",
    "    \n",
    "    def horizontal_barplot(self, figsize=(12, 7), color='firebrick',save_figure=False):\n",
    "\n",
    "        keys, values = self.order_keys_values()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.barh(y=keys, width=values, align='center', color=color, alpha=0.6)\n",
    "        ax.set_yticklabels(keys)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_xlabel(self.y_title)\n",
    "        ax.set_ylabel(self.x_title)\n",
    "        ax.set_title(self.fig_title)\n",
    "        if save_figure:\n",
    "            save_figure_to_disk(main_folder=self.stage, figure_name= self.fig_title , save_as_plt = True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def vertical_barplot(self, figsize=(12, 7), color='firebrick',save_figure=False):\n",
    "\n",
    "        keys, values = self.order_keys_values()\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(x=keys, height=values, color=color, width=0.4, alpha=0.6)\n",
    "\n",
    "        plt.title(self.fig_title)\n",
    "        plt.xlabel(self.x_title)\n",
    "        plt.ylabel(self.y_title)\n",
    "        \n",
    "        if save_figure:\n",
    "            save_figure_to_disk(main_folder=self.stage, figure_name= self.fig_title , save_as_plt = True)\n",
    "        plt.show()\n",
    "\n",
    "    def count_plots(self, cols = list, palette=None, figsize=(14, 48), save_figure=False ):\n",
    "        cols = cols if type(cols) is list else [cols]\n",
    "        plt.figure(figsize=figsize)\n",
    "        count = 1\n",
    "        \n",
    "        for col in cols:\n",
    "            count += 1\n",
    "            plt.subplot(9, 2, count)\n",
    "            sns.countplot(y=col, data=self.input_data, alpha=0.6, order=self.input_data[col].value_counts().index, palette=palette)\n",
    "            count += 1\n",
    "            \n",
    "        if save_figure:\n",
    "            save_figure_to_disk(main_folder=self.stage, figure_name= self.fig_title , save_as_plt = True)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "d = {'a':435454,'b':93849834,'c':93849834}\n",
    "#         self.input_data = input_data\n",
    "#         self.target_label = target_label\n",
    "#         self.asc = asc\n",
    "#         self.fig_title = fig_title\n",
    "#         self.x_title = x_title\n",
    "#         self.y_title = y_title\n",
    "#         self.stage = stage\n",
    "     \n",
    "b = BarPlots(scores ,target_label=target_label, fig_title='barplot',stage= 'Exploratory Data Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8908e1e"
   },
   "source": [
    "## get_probabilities_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec8af591",
    "outputId": "aa34e077-5555-402b-b19f-76da81f3099d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  accuracy: (test=0.293) f1_score: (test=0.246) precision_score: (test=0.272) recall_score: (test=0.246) total time=   0.0s\n",
      "[CV] END  accuracy: (test=0.590) f1_score: (test=0.408) precision_score: (test=0.428) recall_score: (test=0.403) total time=   0.8s\n",
      "[CV] END  accuracy: (test=0.473) f1_score: (test=0.303) precision_score: (test=0.328) recall_score: (test=0.334) total time=   0.5s\n",
      "[CV] END  accuracy: (test=0.498) f1_score: (test=0.355) precision_score: (test=0.372) recall_score: (test=0.357) total time=   2.9s\n",
      "[16:21:32] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:42] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END  accuracy: (test=0.619) f1_score: (test=0.440) precision_score: (test=0.458) recall_score: (test=0.429) total time=  32.2s\n",
      "[CV] END ................................ score: (test=0.293) total time=   0.0s\n",
      "[CV] END ................................ score: (test=0.590) total time=   0.9s\n",
      "[CV] END ................................ score: (test=0.473) total time=   0.3s\n",
      "[16:22:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:06] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ................................ score: (test=0.590) total time=  39.2s\n",
      "[CV 1/3] END ..................n_estimators=200;, score=0.681 total time=  11.8s\n",
      "[CV 2/3] END ..................n_estimators=300;, score=0.682 total time=  18.5s\n",
      "[CV 3/3] END ..................n_estimators=400;, score=0.679 total time=  30.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  accuracy: (test=0.407) f1_score: (test=0.240) precision_score: (test=0.257) recall_score: (test=0.237) total time=   0.1s\n",
      "[CV] END  accuracy: (test=0.363) f1_score: (test=0.291) precision_score: (test=0.304) recall_score: (test=0.315) total time=   0.0s\n",
      "[CV] END  accuracy: (test=0.526) f1_score: (test=0.309) precision_score: (test=0.344) recall_score: (test=0.302) total time=   0.2s\n",
      "[CV] END  accuracy: (test=0.616) f1_score: (test=0.428) precision_score: (test=0.476) recall_score: (test=0.415) total time=   0.8s\n",
      "[16:21:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END  accuracy: (test=0.625) f1_score: (test=0.440) precision_score: (test=0.459) recall_score: (test=0.430) total time=   4.3s\n",
      "[CV] END  accuracy: (test=0.524) f1_score: (test=0.363) precision_score: (test=0.369) recall_score: (test=0.376) total time=   3.0s\n",
      "[16:21:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:41] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END  accuracy: (test=0.616) f1_score: (test=0.439) precision_score: (test=0.457) recall_score: (test=0.431) total time=  31.8s\n",
      "[CV] END ................................ score: (test=0.411) total time=   0.0s\n",
      "[CV] END ................................ score: (test=0.360) total time=   0.0s\n",
      "[CV] END ................................ score: (test=0.495) total time=   0.2s\n",
      "[CV] END ................................ score: (test=0.628) total time=   0.8s\n",
      "[16:22:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ................................ score: (test=0.625) total time=   9.1s\n",
      "[CV] END ................................ score: (test=0.524) total time=   3.8s\n",
      "[CV 1/3] END ..................n_estimators=100;, score=0.678 total time=   5.3s\n",
      "[CV 2/3] END ..................n_estimators=200;, score=0.680 total time=  11.7s\n",
      "[CV 3/3] END ..................n_estimators=300;, score=0.678 total time=  19.8s\n",
      "[CV 1/3] END ..................n_estimators=500;, score=0.685 total time=  32.7s\n",
      "[CV] END  accuracy: (test=0.411) f1_score: (test=0.240) precision_score: (test=0.274) recall_score: (test=0.233) total time=   0.1s\n",
      "[CV] END  accuracy: (test=0.360) f1_score: (test=0.271) precision_score: (test=0.286) recall_score: (test=0.319) total time=   0.0s\n",
      "[CV] END  accuracy: (test=0.495) f1_score: (test=0.293) precision_score: (test=0.420) recall_score: (test=0.304) total time=   0.2s\n",
      "[16:21:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END  accuracy: (test=0.605) f1_score: (test=0.403) precision_score: (test=0.420) recall_score: (test=0.406) total time=   4.9s\n",
      "[CV] END  accuracy: (test=0.456) f1_score: (test=0.166) precision_score: (test=0.163) recall_score: (test=0.194) total time=   0.5s\n",
      "[CV] END  accuracy: (test=0.538) f1_score: (test=0.376) precision_score: (test=0.380) recall_score: (test=0.374) total time=   3.0s\n",
      "[CV] END ................................ score: (test=0.407) total time=   0.0s\n",
      "[CV] END ................................ score: (test=0.491) total time=   0.2s\n",
      "[CV] END ................................ score: (test=0.616) total time=   0.9s\n",
      "[16:22:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ................................ score: (test=0.634) total time=   9.2s\n",
      "[CV] END ................................ score: (test=0.471) total time=   0.4s\n",
      "[CV] END ................................ score: (test=0.538) total time=   3.8s\n",
      "[16:22:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ................................ score: (test=0.625) total time=  40.0s\n",
      "[CV 2/3] END ..................n_estimators=100;, score=0.683 total time=   5.5s\n",
      "[CV 3/3] END ..................n_estimators=200;, score=0.672 total time=  11.7s\n",
      "[CV 1/3] END ..................n_estimators=400;, score=0.683 total time=  25.9s\n",
      "[CV 2/3] END ..................n_estimators=500;, score=0.684 total time=  30.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/carlosdelacruz/miniconda3/envs/autopilot_env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END  accuracy: (test=0.393) f1_score: (test=0.240) precision_score: (test=0.268) recall_score: (test=0.245) total time=   0.1s\n",
      "[CV] END  accuracy: (test=0.491) f1_score: (test=0.298) precision_score: (test=0.301) recall_score: (test=0.305) total time=   0.2s\n",
      "[CV] END  accuracy: (test=0.628) f1_score: (test=0.430) precision_score: (test=0.478) recall_score: (test=0.416) total time=   0.8s\n",
      "[16:21:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END  accuracy: (test=0.634) f1_score: (test=0.449) precision_score: (test=0.460) recall_score: (test=0.441) total time=   5.1s\n",
      "[CV] END  accuracy: (test=0.471) f1_score: (test=0.196) precision_score: (test=0.202) recall_score: (test=0.215) total time=   0.5s\n",
      "[16:21:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:41] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:45] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:21:50] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END  accuracy: (test=0.590) f1_score: (test=0.392) precision_score: (test=0.420) recall_score: (test=0.390) total time=  32.5s\n",
      "[CV] END ................................ score: (test=0.393) total time=   0.0s\n",
      "[CV] END ................................ score: (test=0.363) total time=   0.0s\n",
      "[CV] END ................................ score: (test=0.526) total time=   0.2s\n",
      "[16:22:26] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ................................ score: (test=0.605) total time=   9.1s\n",
      "[CV] END ................................ score: (test=0.456) total time=   0.3s\n",
      "[CV] END ................................ score: (test=0.498) total time=   3.7s\n",
      "[16:22:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:23:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV] END ................................ score: (test=0.610) total time=  39.9s\n",
      "[CV 3/3] END ..................n_estimators=100;, score=0.677 total time=   5.4s\n",
      "[CV 1/3] END ..................n_estimators=300;, score=0.688 total time=  17.2s\n",
      "[CV 2/3] END ..................n_estimators=400;, score=0.686 total time=  29.3s\n",
      "[CV 3/3] END ..................n_estimators=500;, score=0.684 total time=  24.5s\n"
     ]
    }
   ],
   "source": [
    "# UPDATE THIS WITH NEW VERSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "615116a1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0679947"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29beebe6"
   },
   "source": [
    "##  Learning to Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a04cbf63",
    "outputId": "262dec8f-10fc-48c2-9b57-69c5dac2cada"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/by/pt5_hysn0lb75x63vpgqbtsw0000gn/T/ipykernel_19847/797355198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupShuffleSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df['id'])\n",
    "\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "\n",
    "train_data= df.iloc[X_train_inds]\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['id','rank'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['rank'])]\n",
    "\n",
    "test_data= df.iloc[X_test_inds]\n",
    "\n",
    "#We need to keep the id for later predictions\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['rank'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['rank'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dd948fe",
    "outputId": "0d263f7b-f109-48ad-8f6a-09c066f58036"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/by/pt5_hysn0lb75x63vpgqbtsw0000gn/T/ipykernel_19847/3611264344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupShuffleSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 7).split(df, groups=df['id'])\n",
    "\n",
    "X_train_inds, X_test_inds = next(gss)\n",
    "\n",
    "train_data= df.iloc[X_train_inds]\n",
    "X_train = train_data.loc[:, ~train_data.columns.isin(['id','rank'])]\n",
    "y_train = train_data.loc[:, train_data.columns.isin(['rank'])]\n",
    "\n",
    "groups = train_data.groupby('id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "test_data= df.iloc[X_test_inds]\n",
    "\n",
    "#We need to keep the id for later predictions\n",
    "X_test = test_data.loc[:, ~test_data.columns.isin(['rank'])]\n",
    "y_test = test_data.loc[:, test_data.columns.isin(['rank'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "209c20d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebac5f72"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14e77eb6"
   },
   "source": [
    "## AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c0bd1ca"
   },
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = get_splits_wrapper(dataframe = samp_df_encoded, target_label = 'Segment', train_split=True\n",
    "                       ,validation_set=False , test_size = 0.2 , scaled=True,scaler=MinMaxScaler() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d214b47",
    "outputId": "ff164f40-680e-4ea2-87c1-26ccb51e80cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "150/150 [==============================] - 2s 6ms/step - loss: 0.0959 - accuracy: 0.2488 - val_loss: 0.0881 - val_accuracy: 0.1767\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0276 - accuracy: 0.3167 - val_loss: 0.0340 - val_accuracy: 0.2317\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0219 - accuracy: 0.3375 - val_loss: 0.0155 - val_accuracy: 0.2483\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0196 - accuracy: 0.3483 - val_loss: 0.0116 - val_accuracy: 0.2483\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0178 - accuracy: 0.3458 - val_loss: 0.0110 - val_accuracy: 0.3317\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0163 - accuracy: 0.3346 - val_loss: 0.0108 - val_accuracy: 0.3867\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0159 - accuracy: 0.3638 - val_loss: 0.0110 - val_accuracy: 0.4583\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.3613 - val_loss: 0.0086 - val_accuracy: 0.3100\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0144 - accuracy: 0.3654 - val_loss: 0.0088 - val_accuracy: 0.3800\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0146 - accuracy: 0.3625 - val_loss: 0.0080 - val_accuracy: 0.4133\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def encoder(X_train, X_test, epochs = 200, batch_size=32 , optimizer= 'adam', verbose= 1, patience= 3):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    input_n = X_train.shape[1]\n",
    "    param_test = 10\n",
    "  \n",
    "    # define encoder\n",
    "    visible = Input(shape = (input_n,))\n",
    "    \n",
    "    encoder = Dense(input_n*param_test)(visible)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder= LeakyReLU()(encoder)\n",
    "    \n",
    "    encoder = Dense(input_n)(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "\n",
    "    n_bottleneck = round(float(input_n) / 2.0)\n",
    "    bottleneck = Dense(n_bottleneck)(encoder)\n",
    "    \n",
    "    # define decoder \n",
    "    decoder = Dense(input_n)(bottleneck)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    \n",
    "    decoder= Dense(input_n*param_test)(decoder)\n",
    "    decoder = BatchNormalization()(decoder)\n",
    "    decoder= LeakyReLU()(decoder)\n",
    "    \n",
    "    output = Dense(input_n, activation='linear')(decoder)\n",
    "    \n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics = ['accuracy']  )\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor = 'val_accuracy', mode ='max', verbose = verbose, patience = patience)\n",
    "    \n",
    "    history = model.fit(X_train, X_train, epochs = epochs, callbacks = [early_stop], batch_size= batch_size, \n",
    "                        verbose = verbose, \n",
    "                        validation_data=(X_test,X_test))\n",
    "    \n",
    "    encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "encoder_model = encoder(X_train, X_test, epochs = 200, batch_size=16 , verbose= 1, patience= 3  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67c3ce7f"
   },
   "outputs": [],
   "source": [
    "# encode the test and train data\n",
    "\n",
    "X_train_encode = encoder_model.predict(X_train)\n",
    "\n",
    "X_test_encode = encoder_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2bbb42a",
    "outputId": "b9348835-4fe6-4532-93cc-469c0c56c4b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49166666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosdelacruz/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model1 = LogisticRegression()\n",
    "model2 = XGBClassifier()\n",
    "model = model1\n",
    "# fit the model on the training set\n",
    "\n",
    "model.fit(X_train_encode, y_train)\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test_encode)\n",
    "# calculate classification accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46a3470a",
    "outputId": "c67e8799-b5a2-40d9-eb92-9b1c92483df1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5383333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosdelacruz/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test)\n",
    "# calculate classification accuracy\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "debug_funcs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hyperl_env",
   "language": "python",
   "name": "hyperl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
